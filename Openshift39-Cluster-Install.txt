						Deploying OpenShift 3.9  Cluster 
						################################


Infrastructure Setup: 
*********************

Hostname						IP Address			CPUs		RAM				HDD					OS
master.openshift.example.com	192.168.0.111		 2			2 GB		/dev/sda (100 GB) 		CentOS7.5
node1.openshift.example.com		192.168.0.112		 2			2 GB		/dev/sda (100 GB) 		CentOS7.5
node2.openshift.example.com		192.168.0.113		 2			2 GB		/dev/sda (100 GB) 		CentOS7.5
infra.openshift.example.com		192.168.0.114		 2			2 GB		/dev/sda (100 GB) 		CentOS7.5



			
							#################################
							# 		Preparing Nodes: 		#
							#################################

Step 1: Set the hostname:
*************************

[root@master ~]# hostnamectl set-hostname master.openshift.example.com	# For master node


Use the above command to set the hostname for other nodes accordingly. 

[root@master ~]# bash 

Step 2: Configure Static IP Address: 
************************************

[root@master ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33 

TYPE="Ethernet"
BOOTPROTO="none"
IPADDR=192.168.0.111
NETMASK=255.255.255.0
GATEWAY=192.168.0.1
DNS1=192.168.191.2
DEFROUTE="yes"
IPV6INIT="no"
NAME="ens33"
DEVICE="ens33"
ONBOOT="yes"

:wq (save and exit) 

The above IP settings for "master" Node, use the same settings for other nodes accordingly. Don't forget to changes the IP Address for nodes.  

Down the Network Connection, reload the configuration and up it again on all nodes. 

~]# nmcli connection down ens33 

~]# nmcli connection reload

~]# nmcli connection up ens33 


Configure /etc/hosts file for name resolution as following on all nodes: 

~]# vim /etc/hosts

192.168.0.111		master.openshift.example.com	master
192.168.0.112		node1.openshift.example.com		node1
192.168.0.113		node2.openshift.example.com		node2
192.168.0.114		infra.openshift.example.com		infra



Step 3: Use the below command to update the System on all nodes: 

~]# yum update -y

Use the below command to compare the kernel on all nodes: 

~]# echo "Latest Installed Kernel : $(rpm -q kernel --last | head -n 1 | awk '{print $1}')" ; echo "Current Running Kernel  : kernel-$(uname -r)"

If you have different kernel in above command output, then you need to reboot all the system. otherwise jump to Step 4. 

~]# reboot 


Step 4: Once the systems came back UP/ONLINE, Install the following Packages on all nodes:  

~]# yum install -y wget git  nano net-tools docker-1.13.1 bind-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct openssl-devel httpd-tools NetworkManager python-cryptography python-devel python-passlib java-1.8.0-openjdk-headless "@Development Tools"



Step 5: Configure Ansible Repository on master Node only. 
*********************************************************

[root@master ~]# vim /etc/yum.repos.d/ansible.repo

[ansible]
name = Ansible Repo
baseurl = https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/
enabled = 1
gpgcheck =  0


:wq (save and exit) 



Step 6: Start and Enable NetworkManager and Docker Services on all nodes: 
*************************************************************************


~]# systemctl start NetworkManager
~]# systemctl enable NetworkManager
~]# systemctl status NetworkManager

~]# systemctl start docker && systemctl enable docker && systemctl status docker



Step 7: Install Ansible Package and Clone Openshift-Ansible Git Repo on Master Machine :
****************************************************************************************


[root@master ~]# yum -y install ansible-2.6* pyOpenSSL
[root@master ~]# git clone https://github.com/openshift/openshift-ansible.git
[root@master ~]# cd openshift-ansible && git fetch && git checkout release-3.9


Step 8: Generate SSH Keys on Master Node and install it on all nodes: 
*********************************************************************

[root@master ~]# ssh-keygen -f ~/.ssh/id_rsa -N '' 
[root@master ~]#  for host in master.openshift.example.com node1.openshift.example.com node2.openshift.example.com infra.openshift.example.com ; do ssh-copy-id -i ~/.ssh/id_rsa.pub $host; done


Step 9: Now Create Your Own Inventory file for Ansible as following on master Node:  

[root@master ~]# vim ~/inventory.ini 

[OSEv3:children]
masters
nodes
etcd


[masters]
master.openshift.example.com openshift_ip=192.168.0.111

[etcd]
master.openshift.example.com openshift_ip=192.168.0.111

[nodes]
master.openshift.example.com openshift_ip=192.168.0.111 openshift_schedulable=true
infra.openshift.example.com openshift_ip=192.168.0.112 openshift_schedulable=true openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.openshift.example.com openshift_ip=192.168.0.113 openshift_schedulable=true openshift_node_labels="{'region': 'primary', 'zone': 'default'}"
node2.openshift.example.com openshift_ip=192.168.0.114 openshift_schedulable=true openshift_node_labels="{'region': 'primary', 'zone': 'default'}"


[OSEv3:vars]
debug_level=4
ansible_ssh_user=root
enable_excluders=False
enable_docker_excluder=False
openshift_enable_service_catalog=False
ansible_service_broker_install=False

containerized=True
os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'
openshift_disable_check=disk_availability,docker_storage,memory_availability,docker_image_availability

openshift_node_kubelet_args={'pods-per-core': ['10']}

deployment_type=origin
openshift_deployment_type=origin

openshift_release=v3.9.0
openshift_pkg_version=-3.9.0
openshift_image_tag=v3.9.0
openshift_service_catalog_image_version=v3.9.0
template_service_broker_image_version=v3.9.0
osm_use_cockpit=true

# put the router on dedicated infra node
openshift_hosted_router_selector='region=infra'
openshift_master_default_subdomain=apps.openshift.example.com
openshift_public_hostname=master.openshift.example.com

# put the image registry on dedicated infra node
openshift_hosted_registry_selector='region=infra'

# use htpasswd authentication with demo/demo
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]
openshift_master_htpasswd_users={'demo': '$apr1$.MaA77kd$Rlnn6RXq9kCjnEfh5I3w/.'}


:wq (save and exit) 



Step 10: Use the below ansible playbook command to check the prerequisites to deply OpenShift Cluster on master Node: 

[root@master ~]# ansible-playbook -i ~/inventory.ini playbooks/prerequisites.yml


#################################################################################################
#							NOTE: 																#
#	If you want to speed up Openshift Cluster Installation, 									#
#	then you have to pre-pull the below docker images on mentioned nodes. 						#
#	otherwise, the below images will be pulled during the installation. 						#
#																								#
#	docker pull registry.fedoraproject.org/latest/etcd:latest		# Only on master 			#
#	docker pull docker.io/openshift/openvswitch:v3.9.0 				# on all nodes 				#
#	docker pull docker.io/openshift/node:v3.9.0 					# on all nodes 				#
#	docker pull docker.io/openshift/origin:v3.9.0					# on all nodes 				#
#																								#
#################################################################################################


Step 11: Once prerequisites completed without any error use the below ansible playbook to Deploy OpenShift Cluster on master Node: 

[root@master ~]# ansible-playbook -i ~/inventory.ini playbooks/deploy_cluster.yml


Now you have to wait approx 20-30 Minutes to complete the Installation. 


Step 12: Once the Installation is completed, Create a admin user in OpenShift with Password "Pas$$w0rd" from master Node:  

[root@master ~]# ls -l /etc/origin/master/htpasswd
[root@master ~]# cat /etc/origin/master/htpasswd
[root@master ~]# htpasswd /etc/origin/master/htpasswd admin

New password: Pas$$w0rd
Re-type new password: Pas$$w0rd

[root@master ~]# cat /etc/origin/master/htpasswd


Step 13: Use the below command to assign cluster-admin Role to admin user: 

[root@master ~]# oc adm policy add-cluster-role-to-user cluster-admin admin


Step 14: Use the below command to login as admin user on CLI: 

[root@master ~]# oc login

Username: admin 
Password: Pas$$w0rd

[root@master ~]# oc whoami


Step 15: Use below command to list the projects, pods, nodes, Replication Controllers, Services and Deployment Config. 

[root@master ~]# oc get projects

[root@master ~]# oc get nodes 

[root@master ~]# oc get pod --all-namespaces

[root@master ~]# oc get rc 

[root@master ~]# oc get svc 

[root@master ~]# oc get dc



Verifying Multiple etcd Hosts:
##############################

On a master host, verify the etcd cluster health, substituting for the FQDNs of your etcd hosts in
the following:

[root@master ~]# yum install etcd

[root@master ~]# etcdctl -C https://master.openshift.example.com:2379 --ca-file=/etc/origin/master/master.etcd-ca.crt --cert-file=/etc/origin/master/master.etcd-client.crt --key-file=/etc/origin/master/master.etcd-client.key cluster-health

Also verify the member list is correct:

[root@master ~]# etcdctl -C https://master.openshift.example.com:2379 --ca-file=/etc/origin/master/master.etcd-ca.crt --cert-file=/etc/origin/master/master.etcd-client.crt --key-file=/etc/origin/master/master.etcd-client.key member list





Step 16: Now you can access OpenShift Cluster using Web Browser as following: 


https://master.openshift.example.com:8443 

Username: admin
Password: Pas$$w0rd


Step 17: Now Create a Demo Project 

Click on "Create Project"

Name: demo

Display Name: demo

Description: Demo Project

Click on Create. 


Step 18: Now Deploy an application in demo project for testing: 

Click in demo project -> Click on Brows Catalog -> Select PHP -> Next

Version : 7.0 - latest 

Application Name: testapp

Git Repo: https://github.com/sureshchandrarhca15/OpenShift39.git


Next -> Finish. 


Now click on Overview in demo project

See, testapp build is running. so wait for build completion. 

Once build completed, there is a Pod Running for testapp and will have an URL as below: 

http://testapp-demo.apps.openshift.example.com 

Now click on Pod Icon to get the details. 

In my case, this pod is running on node2.openshift.example.com system, so at this point we don't have DNS to resolve App URL. 

I am using /etc/hosts file to resolve it using infra node IP Address. because on infra node our router pod is running so all traffic will redirect from infra node in OpenShift Cluster. 


~]# vim /etc/hosts

#Add the below line 

192.168.191.134	testapp-demo.apps.openshift.example.com

:wq (save and exit) 


Now Click on Pod URL Link and application should be accessible. 

NOTE: You have to configure /etc/hosts file on that system from where you are accessing the Openshift Dashboard. 

 

######################################
